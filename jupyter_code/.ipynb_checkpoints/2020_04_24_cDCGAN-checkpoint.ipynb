{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSC 525 - Deep Learning\n",
    "# Assignment: Final Project\n",
    "# Title: Improving mammography classification with conditional DCGAN generated images.\n",
    "# Team members: Christoph Metzner, Anna-Maria Nau\n",
    "# Date: 04/21/2020\n",
    "\n",
    "# Imported Libaries\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Import script to preprocess the data\n",
    "# noinspection PyUnresolvedReferences\n",
    "# from Data_Preprocessing import preprocess_data\n",
    "# from cDCGAN import generator_model\n",
    "# from cDCGAN import discriminator_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2  # Used in function 'load_datasets'\n",
    "import glob  # Used in function 'load_datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()  # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset\n",
    "# Paths to individual folders containing images regarding classes\n",
    "malignant_folder_path = r\"C:\\Users\\chris\\Desktop\\Studium\\PhD\\Courses\\Spring 2020\\COSC 525 - Deep Learning\\DeepLearning_FinalProject\\Dataset\\Malignant\\\\\"\n",
    "benign_folder_path = r\"C:\\Users\\chris\\Desktop\\Studium\\PhD\\Courses\\Spring 2020\\COSC 525 - Deep Learning\\DeepLearning_FinalProject\\Dataset\\Benign\\\\\"\n",
    "normal_folder_path = r\"C:\\Users\\chris\\Desktop\\Studium\\PhD\\Courses\\Spring 2020\\COSC 525 - Deep Learning\\DeepLearning_FinalProject\\Dataset\\Normal\\\\\"\n",
    "paths = [malignant_folder_path, benign_folder_path, normal_folder_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used for preprocessing the real image dataset\n",
    "# load_datasets: load datasets from directory\n",
    "# create_X_y: generate a complete dataset with images X and respective labels y\n",
    "# preprocess_data: Split X and y into train and test and convert them into tensors type=float32\n",
    "def load_datasets(class_paths):\n",
    "    \"\"\"\n",
    "    Loading and storing information real mammography images in arrays\n",
    "    Categories: normal, malignant, benign\n",
    "    :param class_paths: Array containing three file paths (normal, malignant, benign)\n",
    "    :return: three n*p (p=3) matrices containing information about the three different classes\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    for class_path in class_paths:\n",
    "        dataset = []\n",
    "        for image in glob.glob(class_path + \"*.jpg\"):\n",
    "            dataset.append(cv2.imread(image))\n",
    "        datasets.append(dataset)\n",
    "    return datasets[0], datasets[1], datasets[2]\n",
    "\n",
    "\n",
    "def create_X_y(malignant_data, benign_data, normal_data):\n",
    "    one_hot_encoding = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "    X = malignant_data + benign_data + normal_data\n",
    "    y = len(malignant_data) * [one_hot_encoding[0]] + len(benign_data) * [one_hot_encoding[1]] + len(normal_data) * [\n",
    "        one_hot_encoding[2]]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def preprocess_data(class_paths):\n",
    "    print(\"Loading Dataset...\")\n",
    "\n",
    "    X_malignant, X_benign, X_normal = load_datasets(class_paths=class_paths)\n",
    "    X, y = create_X_y(X_malignant, X_benign, X_normal)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    X_train = np.array(X_train-127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "    X_test = np.array(X_test-127.5) / 127.5 # Normalize the images to [-1, 1]\n",
    "\n",
    "    X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "    y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "    print(\"Finished loading!\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "\n",
    "    # Prepare noise input z\n",
    "    input_z = tf.keras.layers.Input(shape=(100,))\n",
    "    dense_z_1 = tf.keras.layers.Dense(1024 * 4 * 4)(input_z)\n",
    "    act_z_1 = tf.keras.layers.LeakyReLU(alpha=0.2)(dense_z_1)\n",
    "    bn_z_1 = tf.keras.layers.BatchNormalization(momentum=0.9)(act_z_1)\n",
    "    reshape_z = tf.keras.layers.Reshape(target_shape=(4, 4, 1024), input_shape=(4 * 4 * 1024,))(bn_z_1)\n",
    "\n",
    "    # prepare conditional (label) input c\n",
    "    input_c = tf.keras.layers.Input(shape=(3,))\n",
    "    dense_c_1 = tf.keras.layers.Dense(4 * 4 * 1)(input_c)\n",
    "    act_c_1 = tf.keras.layers.LeakyReLU(alpha=0.2)(dense_c_1)\n",
    "    bn_c_1 = tf.keras.layers.BatchNormalization(momentum=0.9)(act_c_1)\n",
    "    reshape_c = tf.keras.layers.Reshape(target_shape=(4, 4, 1), input_shape=(4 * 4 * 1,))(bn_c_1)\n",
    "\n",
    "    # concatenating noise z and label c\n",
    "    concat_z_c = tf.keras.layers.Concatenate()([reshape_z, reshape_c])\n",
    "\n",
    "    # Image generation\n",
    "    # Upsampling to 8x8\n",
    "    conv2D_1 = tf.keras.layers.Conv2DTranspose(filters=1024, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        concat_z_c)\n",
    "    bn_conv2D_1 = tf.keras.layers.BatchNormalization(momentum=0.9)(conv2D_1)\n",
    "    act_conv2D_1 = tf.keras.layers.LeakyReLU(alpha=0.2)(bn_conv2D_1)\n",
    "\n",
    "    # Upsampling to 16x16\n",
    "    conv2D_2 = tf.keras.layers.Conv2DTranspose(filters=512, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        act_conv2D_1)\n",
    "    bn_conv2D_2 = tf.keras.layers.BatchNormalization(momentum=0.9)(conv2D_2)\n",
    "    act_conv2D_2 = tf.keras.layers.LeakyReLU(alpha=0.2)(bn_conv2D_2)\n",
    "\n",
    "\n",
    "    # Upsampling to 32x32\n",
    "    conv2D_3 = tf.keras.layers.Conv2DTranspose(filters=256, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        act_conv2D_2)\n",
    "    bn_conv2D_3 = tf.keras.layers.BatchNormalization(momentum=0.9)(conv2D_3)\n",
    "    act_conv2D_3 = tf.keras.layers.LeakyReLU(alpha=0.2)(bn_conv2D_3)\n",
    "\n",
    "    # Upsampling to 64x64\n",
    "    conv2D_4 = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        act_conv2D_3)\n",
    "    bn_conv2D_4 = tf.keras.layers.BatchNormalization(momentum=0.9)(conv2D_4)\n",
    "    act_conv2D_4 = tf.keras.layers.LeakyReLU(alpha=0.2)(bn_conv2D_4)\n",
    "\n",
    "    # Upsampling to 128x128\n",
    "    conv2D_5 = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        act_conv2D_4)\n",
    "    bn_conv2D_5 = tf.keras.layers.BatchNormalization(momentum=0.9)(conv2D_5)\n",
    "    act_conv2D_5 = tf.keras.layers.LeakyReLU(alpha=0.2)(bn_conv2D_5)\n",
    "\n",
    "    # Upsampling to 256x256\n",
    "    conv2D_6 = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        act_conv2D_5)\n",
    "    bn_conv2D_6 = tf.keras.layers.BatchNormalization(momentum=0.9)(conv2D_6)\n",
    "    act_conv2D_6 = tf.keras.layers.LeakyReLU(alpha=0.2)(bn_conv2D_6)\n",
    "\n",
    "    # Upsampling to 512x512\n",
    "    conv2D_7 = tf.keras.layers.Conv2DTranspose(filters=16, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        act_conv2D_6)\n",
    "    bn_conv2D_7 = tf.keras.layers.BatchNormalization(momentum=0.9)(conv2D_7)\n",
    "    act_conv2D_7 = tf.keras.layers.LeakyReLU(alpha=0.2)(bn_conv2D_7)\n",
    "\n",
    "    # Upsampling to 1024x1024\n",
    "    conv2D_8 = tf.keras.layers.Conv2DTranspose(filters=8, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        act_conv2D_7)\n",
    "    bn_conv2D_8 = tf.keras.layers.BatchNormalization(momentum=0.9)(conv2D_8)\n",
    "    act_conv2D_8 = tf.keras.layers.LeakyReLU(alpha=0.2)(bn_conv2D_8)\n",
    "\n",
    "    # Output layer\n",
    "    conv2D_9 = tf.keras.layers.Conv2D(filters=3, kernel_size=(1, 1), strides=(1, 1), activation='tanh', padding='same')(\n",
    "        act_conv2D_8)\n",
    "\n",
    "    # Model output\n",
    "    model = tf.keras.models.Model(inputs=[input_z, input_c], outputs=conv2D_9)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "\n",
    "    # Prepare noise input z\n",
    "    input_z = tf.keras.layers.Input(shape=(100,))\n",
    "    dense_z_1 = tf.keras.layers.Dense(1024 * 4 * 4)(input_z)\n",
    "    act_z_1 = tf.keras.layers.LeakyReLU(alpha=0.2)(dense_z_1)\n",
    "    bn_z_1 = tf.keras.layers.BatchNormalization(momentum=0.9)(act_z_1)\n",
    "    reshape_z = tf.keras.layers.Reshape(target_shape=(4, 4, 1024), input_shape=(4 * 4 * 1024,))(bn_z_1)\n",
    "\n",
    "    # prepare conditional (label) input c\n",
    "    input_c = tf.keras.layers.Input(shape=(3,))\n",
    "    dense_c_1 = tf.keras.layers.Dense(4 * 4 * 1)(input_c)\n",
    "    act_c_1 = tf.keras.layers.LeakyReLU(alpha=0.2)(dense_c_1)\n",
    "    bn_c_1 = tf.keras.layers.BatchNormalization(momentum=0.9)(act_c_1)\n",
    "    reshape_c = tf.keras.layers.Reshape(target_shape=(4, 4, 1), input_shape=(4 * 4 * 1,))(bn_c_1)\n",
    "\n",
    "    # concatenating noise z and label c\n",
    "    concat_z_c = tf.keras.layers.Concatenate()([reshape_z, reshape_c])\n",
    "\n",
    "    # Image generation\n",
    "    # Upsampling to 8x8\n",
    "    conv2D_1 = tf.keras.layers.Conv2DTranspose(filters=1024, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        concat_z_c)\n",
    "    act_conv2D_1 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv2D_1)\n",
    "\n",
    "    # Upsampling to 16x16\n",
    "    conv2D_2 = tf.keras.layers.Conv2DTranspose(filters=512, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        act_conv2D_1)\n",
    "    act_conv2D_2 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv2D_2)\n",
    "\n",
    "    # Upsampling to 32x32\n",
    "    conv2D_3 = tf.keras.layers.Conv2DTranspose(filters=256, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        act_conv2D_2)\n",
    "    act_conv2D_3 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv2D_3)\n",
    "\n",
    "    # Upsampling to 64x64\n",
    "    conv2D_4 = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        act_conv2D_3)\n",
    "    act_conv2D_4 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv2D_4)\n",
    "\n",
    "    # Upsampling to 128x128\n",
    "    conv2D_5 = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        act_conv2D_4)\n",
    "    act_conv2D_5 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv2D_5)\n",
    "\n",
    "    # Upsampling to 256x256\n",
    "    conv2D_6 = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        act_conv2D_5)\n",
    "    act_conv2D_6 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv2D_6)\n",
    "\n",
    "    # Upsampling to 512x512\n",
    "    conv2D_7 = tf.keras.layers.Conv2DTranspose(filters=16, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        act_conv2D_6)\n",
    "    act_conv2D_7 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv2D_7)\n",
    "\n",
    "    # Upsampling to 1024x1024\n",
    "    conv2D_8 = tf.keras.layers.Conv2DTranspose(filters=8, kernel_size=(3, 3), strides=(2, 2), padding='same')(\n",
    "        act_conv2D_7)\n",
    "    act_conv2D_8 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv2D_8)\n",
    "\n",
    "    # Output layer\n",
    "    conv2D_9 = tf.keras.layers.Conv2D(filters=3, kernel_size=(1, 1), strides=(1, 1), activation='tanh', padding='same')(\n",
    "        act_conv2D_8)\n",
    "\n",
    "    # Model output\n",
    "    model = tf.keras.models.Model(inputs=[input_z, input_c], outputs=conv2D_9)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    # prepare conditional (label) input c\n",
    "    input_c = tf.keras.layers.Input(shape=(3,))\n",
    "    dense_c_1 = tf.keras.layers.Dense(1024 * 1024 * 1)(input_c)\n",
    "    act_c_1 = tf.keras.layers.LeakyReLU(alpha=0.2)(dense_c_1)\n",
    "    bn_c_1 = tf.keras.layers.BatchNormalization(momentum=0.9)(act_c_1)\n",
    "    reshape_c = tf.keras.layers.Reshape(target_shape=(1024, 1024, 1), input_shape=(1024 * 1024 * 1,))(bn_c_1)\n",
    "\n",
    "    # Get input images x: real p(x_r) or fake p(x_z)\n",
    "    input_x = tf.keras.layers.Input(shape=(1024, 1024, 3))\n",
    "\n",
    "    # Concatenate input c and image x\n",
    "    concat_x_c = tf.keras.layers.Concatenate()([input_x, reshape_c])\n",
    "\n",
    "    # Feature extraction for discriminating real from fake images\n",
    "    # Begin feature extraction process\n",
    "    # Downsampling: 16 feature maps\n",
    "    conv2d_1 = tf.keras.layers.Conv2D(16, (3, 3), strides=(2, 2), padding='same', name='conv_512x512')(concat_x_c)\n",
    "    act_conv2d_1 = tf.keras.layers.LeakyReLU(alpha=0.2, name='lReLU_512x512')(conv2d_1)\n",
    "    dp_conv2d_1 = tf.keras.layers.Dropout(0.33, name='Dropout_512x512')(act_conv2d_1)\n",
    "\n",
    "    # Downsampling: 32 feature maps\n",
    "    conv2d_2 = tf.keras.layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', name='conv_256x256')(dp_conv2d_1)\n",
    "    act_conv2d_2 = tf.keras.layers.LeakyReLU(alpha=0.2, name='lReLU_256x256')(conv2d_2)\n",
    "    dp_conv2d_2 = tf.keras.layers.Dropout(0.33, name='Dropout_256x256')(act_conv2d_2)\n",
    "\n",
    "    # Downsampling: 64 feature maps\n",
    "    conv2d_3 = tf.keras.layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', name='conv_128x128')(dp_conv2d_2)\n",
    "    act_conv2d_3 = tf.keras.layers.LeakyReLU(alpha=0.2, name='lReLU_128x128')(conv2d_3)\n",
    "    dp_conv2d_3 = tf.keras.layers.Dropout(0.33, name='Dropout_128x128')(act_conv2d_3)\n",
    "\n",
    "    # Downsampling: 128 feature maps\n",
    "    conv2d_4 = tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same', name='conv_64x64')(dp_conv2d_3)\n",
    "    act_conv2d_4 = tf.keras.layers.LeakyReLU(alpha=0.2, name='lReLU_64x64')(conv2d_4)\n",
    "    dp_conv2d_4 = tf.keras.layers.Dropout(0.33, name='Dropout_64x64')(act_conv2d_4)\n",
    "\n",
    "    # Downsampling: 256 feature maps\n",
    "    conv2d_5 = tf.keras.layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same', name='conv_32x32')(dp_conv2d_4)\n",
    "    act_conv2d_5 = tf.keras.layers.LeakyReLU(alpha=0.2, name='lReLU_32x32')(conv2d_5)\n",
    "    dp_conv2d_5 = tf.keras.layers.Dropout(0.33, name='Dropout_32x32')(act_conv2d_5)\n",
    "\n",
    "    # Downsampling: 512 feature maps\n",
    "    conv2d_6 = tf.keras.layers.Conv2D(512, (3, 3), strides=(2, 2), padding='same', name='conv_16x16')(dp_conv2d_5)\n",
    "    act_conv2d_6 = tf.keras.layers.LeakyReLU(alpha=0.2, name='lReLU_16x16')(conv2d_6)\n",
    "    dp_conv2d_6 = tf.keras.layers.Dropout(0.33, name='Dropout_16x16')(act_conv2d_6)\n",
    "\n",
    "    # Downsampling: 512 feature maps\n",
    "    conv2d_7 = tf.keras.layers.Conv2D(512, (3, 3), strides=(2, 2), padding='same', name='conv_8x8')(dp_conv2d_6)\n",
    "    act_conv2d_7 = tf.keras.layers.LeakyReLU(alpha=0.2, name='lReLU_8x8')(conv2d_7)\n",
    "    dp_conv2d_7 = tf.keras.layers.Dropout(0.33, name='Dropout_8x8')(act_conv2d_7)\n",
    "\n",
    "    # Downsampling: 512 feature maps\n",
    "    conv2d_8 = tf.keras.layers.Conv2D(512, (3, 3), strides=(2, 2), padding='same', name='conv_4x4')(dp_conv2d_7)\n",
    "    act_conv2d_8 = tf.keras.layers.LeakyReLU(alpha=0.2, name='lReLU_4x4')(conv2d_8)\n",
    "    dp_conv2d_8 = tf.keras.layers.Dropout(0.33, name='Dropout_4x4')(act_conv2d_8)\n",
    "\n",
    "    # Downsampling: 512 feature maps\n",
    "    conv2d_9 = tf.keras.layers.Conv2D(512, (4, 4), strides=(1, 1), padding='valid', name='conv_1x1')(dp_conv2d_8)\n",
    "    act_conv2d_9 = tf.keras.layers.LeakyReLU(alpha=0.2, name='lReLU_1x1')(conv2d_9)\n",
    "    dp_conv2d_9 = tf.keras.layers.Dropout(0.33, name='Dropout_1x1')(act_conv2d_9)\n",
    "\n",
    "    flat_output = tf.keras.layers.Flatten()(dp_conv2d_9)\n",
    "    final_output = tf.keras.layers.Dense(units=1, activation='sigmoid', name='final_output')(flat_output)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_x, input_c], outputs=final_output, name=\"Discriminator\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions for loss of discriminator and generator\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    # This method returns a helper function to compute cross entropy loss\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # This method returns a helper function to compute cross entropy loss\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating generator and discrimantor object\n",
    "generator = generator_model()\n",
    "discriminator = discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_size = 1\n",
    "noise_z_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.28843427  0.7865531  -0.0024817  -0.85950875  1.5919294  -1.6345774\n",
      "  -0.7113903  -2.4179845   0.2553581  -0.13223639 -0.5814099   0.14731129\n",
      "  -0.22042246  0.8065903  -1.9191834  -0.91014594  1.1145893   1.168937\n",
      "   0.7275707   0.1357839  -1.1026635  -1.155575    0.79546547 -0.34324825\n",
      "   0.99121237 -0.51971865 -1.586511   -1.7908614   0.20854546 -0.31745642\n",
      "  -0.09367558  0.728404    0.6294257   0.18486251  1.129683   -0.23667346\n",
      "  -0.7851111   0.49263448 -0.7431626   1.0770707   0.5583076  -1.1763955\n",
      "  -0.956689   -0.23351373  1.6854839   0.10741682  1.4396471  -0.50569075\n",
      "   1.2770212  -0.18282098 -0.02687307 -1.5118371  -0.1623585   1.4467642\n",
      "  -1.1270257   1.1933036   0.38774043 -0.7211532  -2.278505   -0.4819333\n",
      "  -1.3955042  -0.37239257  0.24678746 -0.6852328   0.7505901  -0.14665996\n",
      "   0.22154938 -0.7528595   0.09606937  0.9864514   0.15396734  1.8891131\n",
      "   0.3997921  -1.6635398   1.1111935  -0.22371708 -2.0507278  -0.3853559\n",
      "   0.58353424 -1.3612685  -1.3236498   1.1629744   1.0119823  -0.2472472\n",
      "  -1.529254   -0.9476313   0.5630125   0.30500206  2.0677686   2.0810907\n",
      "   1.6326352  -0.00973048  1.6464461  -0.81612176 -0.30402336  0.77065367\n",
      "   0.7801303  -1.7441524   0.96427006  0.4279345 ]], shape=(1, 100), dtype=float32)\n",
      "[1]\n",
      "tf.Tensor([[0. 1. 0.]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "noise_z = tf.random.normal([Batch_size, noise_z_dim])\n",
    "print(noise_z)\n",
    "# Fake labels\n",
    "rnd_sample_labels = np.random.randint(0, 3, Batch_size)\n",
    "print(rnd_sample_labels)\n",
    "# generate one_hot_encoding\n",
    "fake_labels = tf.one_hot(indices=rnd_sample_labels, depth=3, dtype=tf.float32)\n",
    "print(fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_images = generator(inputs=[noise_z, fake_labels], training=True)\n",
    "plt.imshow(fake_images[0, :28, :28, 0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
